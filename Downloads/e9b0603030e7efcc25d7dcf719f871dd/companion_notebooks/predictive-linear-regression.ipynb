{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Linear Regression\n",
    "\n",
    "Brandon Martin-Anderson June 2019, with great debt to Moses Marsh, Matt Drury\n",
    "\n",
    "### Learning goals\n",
    "\n",
    "- Describe the essential aspects of a prediction task.\n",
    "- Why estimate?\n",
    "- How do we test a model?\n",
    "- Fit a linear model and evaluate its performance with and without `sklearn`.\n",
    "- Use multiple features, categorical features, and engineered features.\n",
    "- Pick polynomial regression order using cross-validation.\n",
    "\n",
    "### The task of predicting quantities: in story form\n",
    "\n",
    "Let's start a business: **we estimate the fuel economy of cars.** People describe a car and pay us money, and we tell them the fuel economy (in miles per gallon) of their car.\n",
    "\n",
    "- The first customer comes in, and says, \"This is a very competitive field. Why should we pay you?\"\n",
    "- You say, \"We are very good at what we do.\"\n",
    "- They say, \"Your competitor says the same thing. Do you have evidence?\"\n",
    "\n",
    "Note two things.\n",
    "\n",
    "1. **You are competing against other models.**\n",
    "2. **Your model will be evaluated quantitatively.**\n",
    "\n",
    "So you say: \"We have analyzed all of our competitors's predictions. All their predictions have some error; the mean of their predictive errors is 5 mpg. We can do better.\"\n",
    "\n",
    "- Them: \"Ah, interesting. Prove it.\"\n",
    "- You: \"Describe a car.\"\n",
    "- Them: \"A 2020 hybrid Ford F250\"\n",
    "- You: \"Can't predict - that **that car isn't in our records**\"\n",
    "- Them: \"What good is that?\"\n",
    "- You: \"If you ask about an old car, we can predict it perfectly.\"\n",
    "- Them: \"And...why would we pay for that?\"\n",
    "- You: \"I don't know, but we are perfect.\"\n",
    "- Them: \"We'll go to your competitor instead.\"\n",
    "\n",
    "3. **The only performance that matters is predictive performance on unseen examples.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple predictive model\n",
    "\n",
    "We wish to devise a predictive model $\\hat{f}$ for our fuel-economy-estimation business. We start with a CSV we found on the internet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"data/cars.csv\", na_values=[\"?\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a very simple model: **always predict the average mpg**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model\n",
    "\n",
    "Let's evaluate our model.\n",
    "\n",
    "Because our metric for success is performance on unseen data, we need to keep some our data unseen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(cars)\n",
    "n_holdout = int(n*0.2)\n",
    "print(f\"{n} records total, holding out {n_holdout}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the cars using [.sample()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html); or else we'd learn just about the 8-cylinder cars (they're ordered in the dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars.sample(len(cars))\n",
    "cars_test = cars.iloc[:n_holdout]\n",
    "cars_train = cars.iloc[n_holdout:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_train.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the \"model\" simply involves finding the mean, and returning it for every row in $X$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mpg = cars_train.mpg.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fhat(X): return np.ones(len(X))*mean_mpg  # our model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = fhat(cars_test)  # predictions for all cars!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the MSE without loops using vectorized numpy-style operations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((cars_test.mpg - yhat)**2).mean()\n",
    "mse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rough estimate, but it satisfies two important requirements:\n",
    "\n",
    "- It is a quantitative, comparable error metric,\n",
    "- computed on unseen data.\n",
    "\n",
    "In a very simple way, we've done our job. Simple models like this are called **benchmarks**; they're very useful for setting a lower bound on the performance of more complex models. **This is our number to beat**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though MSE is mathematically elegant, it can be difficult to interpret.\n",
    "It is also common to look at the square root of the MSE (RMSE), which has\n",
    "the same units as the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse**0.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be interpreted as how far off the model is on average. So on average (roughly, 1 standard deviation or less away), our estimate of 24 mpg is off from the actual mpg in our test data by 8 mpg.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression as linear algebra\n",
    "\n",
    "The mpg (y) can be modeled as a function of the car weight (X)\n",
    "\n",
    "$$\\hat{y_i} = mx_i + b$$\n",
    "\n",
    "We can express this as\n",
    "\n",
    "$$\\hat{y} = X\\beta$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\hat{y_i}$ is the ith item in $\\hat{y}$\n",
    "- $x_i$ is the ith row in $X$\n",
    "- $\\beta$ is $(m,b)$\n",
    "\n",
    "This is a small notational convenience that will become a huge convenience soon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([cars_train.weight.values, np.ones_like(cars_train.weight)]).T\n",
    "y = cars_train.mpg.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y  # was not aware of @ as dot product\n",
    "beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = beta  # slope, intercept\n",
    "def fhat(X): return m*X + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "ax.scatter(cars_test.weight, cars_test.mpg, label=\"data\")\n",
    "ax.set_xlabel(\"weight (lbs)\")\n",
    "ax.set_ylabel(\"fuel economy (mpg)\")\n",
    "ax.set_title(\"test set fuel economy data & predictive model\")\n",
    "\n",
    "weights = np.linspace(1500, 5000)\n",
    "ax.plot(weights, fhat(weights), c=\"red\", label=\"model\")\n",
    "ax.legend()\n",
    "\n",
    "for x_i, y_i in zip(cars_test.weight, cars_test.mpg):\n",
    "    plt.plot([x_i, x_i], [y_i, m*x_i+b], color='gray', linestyle='dashed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = fhat(cars_test.weight)\n",
    "mse = ((cars_test.mpg - yhat)**2).mean()\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've halved our RMSE, a stunning success using linear regression!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression on multiple predictors\n",
    "\n",
    "As a notational convenience we coined:\n",
    "\n",
    "$$\\hat{y} = X\\beta$$\n",
    "\n",
    "We can generalize $X$ to a n by p+1 matrix, where row i corresponds to the ith sample, and column j corresponds to the jth predictor. Likewise $\\beta$ is a vector of length p+1.\n",
    "\n",
    "If we crack this open, it expands to:\n",
    "\n",
    "$$ \\hat{y*i} = \\beta_0 \\cdot 1 + \\beta_1x*{i,1} + \\beta*2x*{i,2} + ... + \\beta*px*{i,p} $$\n",
    "\n",
    "For example,\n",
    "\n",
    "- 0th column would be all 1s,\n",
    "- the 1st column of X holds the car weight,\n",
    "- the 2st column of X holds the cylinder displacement.\n",
    "\n",
    "In this case the semantics of $\\beta$ is:\n",
    "\n",
    "- $\\beta_0$ is the \"base\" car mpg\n",
    "- $\\beta_1$ is the mpg penalty for each pound of car\n",
    "- $\\beta_2$ is the mpg penalty for each cc of engine displacement\n",
    "\n",
    "Let's trust the math and try it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([np.ones_like(cars_train.weight),\n",
    "              cars_train.weight.values,\n",
    "              cars_train.displacement.values]).T\n",
    "y = cars_train.mpg.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y  # the closed-form OLS solution\n",
    "beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.stack([np.ones_like(cars_test.weight),\n",
    "                   cars_test.weight.values,\n",
    "                   cars_test.displacement.values]).T\n",
    "y_test = cars_test.mpg.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((X_test @ beta - y_test)**2).mean()\n",
    "rmse = np.sqrt(mse)\n",
    "print(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding displacement improves performance yet again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using categorical predictors\n",
    "\n",
    "In the cars dataset, the feature `origin` corresponds to the _continent_ or origin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.origin.value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a numerical feature, but it isn't at all - the codes correspond to the USA, Europe, and Asia in increasing order. How do we use the tools of linear regression on a feature that's _not a number_?\n",
    "\n",
    "One simple strategy is to create numerical \"dummy variables\" that encode the presence of a categorical feature with a 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars[\"american\"] = cars.origin.map(lambda x: 1.0 if x == 1 else 0.0)\n",
    "cars[\"european\"] = cars.origin.map(lambda x: 1.0 if x == 2 else 0.0)\n",
    "cars[\"asian\"] = cars.origin.map(lambda x: 1.0 if x == 3 else 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there exists a linear relationship between the category-dummy and the\n",
    "target variable. For example as a car's \"american\" property goes to 1.0,\n",
    "the fuel economy goes down:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cars.american, cars.mpg)\n",
    "plt.xlabel(\"American-ness\")\n",
    "plt.ylabel(\"fuel economy (mpg)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note only using \"American or not\" in model below\n",
    "cars_test = cars.iloc[:n_holdout]\n",
    "cars_train = cars.iloc[n_holdout:]\n",
    "\n",
    "X = np.stack([np.ones_like(cars_train.weight),\n",
    "              cars_train.weight.values,\n",
    "              cars_train.displacement.values,\n",
    "              cars_train.american.values]).T\n",
    "y = cars_train.mpg.values\n",
    "\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "X_test = np.stack([np.ones_like(cars_test.weight),\n",
    "                   cars_test.weight.values,\n",
    "                   cars_test.displacement.values,\n",
    "                   cars_test.american.values]).T\n",
    "y_test = cars_test.mpg.values\n",
    "\n",
    "mse = ((X_test @ beta - y_test)**2).mean()\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we were able to squeak out a small improvement on the test set.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineered features: making curves out of straight lines\n",
    "\n",
    "The relationship between weight and fuel economy is _not a straight line_. This makes sense if you think about it: the fuel economy of a car can never be negative - it will go to zero as the weight increases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "ax.scatter(cars.weight, cars.mpg)\n",
    "ax.set_xlabel(\"weight (lbs)\")\n",
    "ax.set_ylabel(\"fuel economy (mpg)\")\n",
    "ax.set_title(\"car weight vs. fuel economy\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is to perform linear regression on engineered features. One potential model:\n",
    "\n",
    "$$y = \\beta_0 + \\frac{\\beta_1}{x_{weight}}$$\n",
    "\n",
    "might seem nonlinear, but it's simply a linear function of the predictor $\\frac{1}{x_{weight}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([np.ones_like(cars_train.weight),\n",
    "              1/cars_train.weight.values]).T\n",
    "y = cars_train.mpg.values\n",
    "\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "beta\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "ax.scatter(cars.weight, cars.mpg)\n",
    "weightspace = np.linspace(1500, 5000)\n",
    "ax.plot(weightspace, beta[1]*(1/weightspace)+beta[0], c=\"red\")\n",
    "ax.set_xlabel(\"weight (lbs)\")\n",
    "ax.set_ylabel(\"fuel economy (mpg)\")\n",
    "ax.set_title(\"car weight vs. fuel economy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.stack([np.ones_like(cars_test.weight),\n",
    "                   1/cars_test.weight.values]).T\n",
    "y_test = cars_test.mpg.values\n",
    "\n",
    "\n",
    "mse = ((X_test @ beta - y_test)**2).mean()\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular strategy for fitting complex curves is **polynomial regression**, which involves creating polynomial features from observed features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going up to cubic features\n",
    "\n",
    "X = np.stack([np.ones_like(cars_train.weight),\n",
    "              cars_train.weight.values**1,\n",
    "              cars_train.weight.values**2,\n",
    "              cars_train.weight.values**3]).T\n",
    "y = cars_train.mpg.values\n",
    "\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "ax.scatter(cars.weight, cars.mpg)\n",
    "weightspace = np.linspace(1500, 5000)\n",
    "fake_feats = np.stack([np.ones_like(weightspace),\n",
    "                       weightspace**1,\n",
    "                       weightspace**2,\n",
    "                       weightspace**3]).T\n",
    "ax.plot(weightspace, fake_feats @ beta, c=\"red\")\n",
    "ax.set_xlabel(\"weight (lbs)\")\n",
    "ax.set_ylabel(\"fuel economy (mpg)\")\n",
    "ax.set_title(\"car weight vs. fuel economy\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: which do you think is a preferable featurization in this case? 1/weight or polynomial approach?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation for hyperparameter selection\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to go overboard with polynomial features, leading to poor generalization on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 3  # try 3, 5, 8, 10\n",
    "\n",
    "features = [np.ones_like(cars_train.weight)]\n",
    "for i in range(1, order+1):\n",
    "    features.append(cars_train.weight.values**i)\n",
    "X = np.stack(features).T\n",
    "y = cars_train.mpg.values\n",
    "\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "ax.scatter(cars.weight, cars.mpg)\n",
    "weightspace = np.linspace(1500, 5000)\n",
    "fake_feats = [np.ones_like(weightspace)]\n",
    "for i in range(1, order+1):\n",
    "    fake_feats.append(weightspace**i)\n",
    "fake_feats = np.stack(fake_feats).T\n",
    "ax.plot(weightspace, fake_feats @ beta, c=\"red\")\n",
    "ax.set_xlabel(\"weight (lbs)\")\n",
    "ax.set_ylabel(\"fuel economy (mpg)\")\n",
    "ax.set_title(\"car weight vs. fuel economy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = range(1, 10)\n",
    "mses = []\n",
    "for order in orders:\n",
    "\n",
    "    features = [np.ones_like(cars_train.weight)]\n",
    "    for i in range(1, order+1):\n",
    "        features.append(cars_train.weight.values**i)\n",
    "    X = np.stack(features).T\n",
    "    y = cars_train.mpg.values\n",
    "\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    features = [np.ones_like(cars_test.weight)]\n",
    "    for i in range(1, order+1):\n",
    "        features.append(cars_test.weight.values**i)\n",
    "    X_test = np.stack(features).T\n",
    "\n",
    "    test_mse = ((X_test @ beta - y_test)**2).mean()\n",
    "    mses.append(test_mse)\n",
    "\n",
    "plt.plot(orders, mses)\n",
    "plt.xlabel(\"polynomial order\")\n",
    "plt.ylabel(\"test MSE\")\n",
    "plt.title(\"Polynomial out of sample error\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In practice: linear regression using `sklearn`\n",
    "\n",
    "Mostly we just use helper functions to do all of this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together feature matrix and target vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cars[[\"cylinders\", \"displacement\", \"horsepower\", \"weight\",\n",
    "          \"acceleration\", \"american\", \"european\", \"asian\"]]\n",
    "y = cars[\"mpg\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit using training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using the test features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate against the test target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, yhat)\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ The best yet. This model \"wins\" based on performance, but what influences mpg the most? Ah, if you care about that you're in the wrong place! What you want is inferential regression (and that's next).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jumping ahead: inference\n",
    "\n",
    "Some assumptions that must hold in order to use linear regression for _inference_.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity: violated when you see nonlinear trends in your data / residuals\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoscedasticity: violated when the variance of your residuals isn't constant\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality: violated when the residuals are not normally distributed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity: strictly violated when one feature is a linear combination of others, loosely violated when one feature is highly correlated with others\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
